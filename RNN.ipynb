{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwmN2Q4ChV9jlcA4rlo3fn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianellis1997/Sarcasm_Detection/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hj1SI-NHC7gn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysUVbWxChIF",
        "outputId": "6a9747a7-2a59-4198-a2aa-33046df8090e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sarcasm_Detection'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 224 (delta 127), reused 115 (delta 62), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (224/224), 4.42 MiB | 11.61 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/brianellis1997/Sarcasm_Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU6jdiVJCq6f",
        "outputId": "a5a8e8a0-7a6f-4e40-f168-d96f9e7defb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_bal = pd.read_csv('/content/drive/MyDrive/Sarcasm_Data/Train_Balanced.csv')   # Make sure path is correct in your google drive\n",
        "train_bal.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZPEYSvW3CvzL",
        "outputId": "941bcb5b-5869-43af-8e01-e40e6d061e66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  label                                            comment  \\\n",
              "0      522142      0  I personally wasn't a huge Garrosh fan, I've a...   \n",
              "1      907864      1                                     you forgot the   \n",
              "2      604170      1                 Nah man, she's clearly an ad carry   \n",
              "3      110635      1                            This sub in a nutshell.   \n",
              "4      997758      0                                       Yes... I do.   \n",
              "\n",
              "        author        subreddit  score  ups  downs        date  \\\n",
              "0  cromemako83        AskReddit      2    2      0  2015-07-01   \n",
              "1  _SharkWeek_        AskReddit      1    1      0  2013-03-01   \n",
              "2    jdswift13  leagueoflegends      1    1      0  2015-10-01   \n",
              "3    trickz-M-  GlobalOffensive      1   -1     -1  2016-12-01   \n",
              "4    guriboysf           videos      4    4      0  2010-01-01   \n",
              "\n",
              "           created_utc                                     parent_comment  \n",
              "0  2015-07-11 01:55:53  Fuck Vol'jin. Garrosh Hellscream did nothing w...  \n",
              "1  2013-03-14 03:03:46  That's a lie fed to you by the LIEberal media....  \n",
              "2  2015-10-21 23:22:17                                  she isnt already?  \n",
              "3  2016-12-05 03:50:18            Cloud 9 Qualify! (ONLY C9 FANS ALLOWED(  \n",
              "4  2010-01-17 21:32:40                  \"so, i hear you have a fat cock.\"  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0498cf28-db33-407c-85d2-1fb5b2aa0fb7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>522142</td>\n",
              "      <td>0</td>\n",
              "      <td>I personally wasn't a huge Garrosh fan, I've a...</td>\n",
              "      <td>cromemako83</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2015-07-01</td>\n",
              "      <td>2015-07-11 01:55:53</td>\n",
              "      <td>Fuck Vol'jin. Garrosh Hellscream did nothing w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>907864</td>\n",
              "      <td>1</td>\n",
              "      <td>you forgot the</td>\n",
              "      <td>_SharkWeek_</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2013-03-01</td>\n",
              "      <td>2013-03-14 03:03:46</td>\n",
              "      <td>That's a lie fed to you by the LIEberal media....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>604170</td>\n",
              "      <td>1</td>\n",
              "      <td>Nah man, she's clearly an ad carry</td>\n",
              "      <td>jdswift13</td>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2015-10-01</td>\n",
              "      <td>2015-10-21 23:22:17</td>\n",
              "      <td>she isnt already?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110635</td>\n",
              "      <td>1</td>\n",
              "      <td>This sub in a nutshell.</td>\n",
              "      <td>trickz-M-</td>\n",
              "      <td>GlobalOffensive</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-12-01</td>\n",
              "      <td>2016-12-05 03:50:18</td>\n",
              "      <td>Cloud 9 Qualify! (ONLY C9 FANS ALLOWED(</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>997758</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes... I do.</td>\n",
              "      <td>guriboysf</td>\n",
              "      <td>videos</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2010-01-01</td>\n",
              "      <td>2010-01-17 21:32:40</td>\n",
              "      <td>\"so, i hear you have a fat cock.\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0498cf28-db33-407c-85d2-1fb5b2aa0fb7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0498cf28-db33-407c-85d2-1fb5b2aa0fb7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0498cf28-db33-407c-85d2-1fb5b2aa0fb7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42826fd5-6c70-4c86-b4a6-b18daa8703bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42826fd5-6c70-4c86-b4a6-b18daa8703bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42826fd5-6c70-4c86-b4a6-b18daa8703bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_bal"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from gensim.models import KeyedVectors\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Load GloVe word vectors\n",
        "# glove_file = '/content/drive/MyDrive/Sarcasm_Data/glove.6B.100d.txt'  # Adjust the path to your downloaded GloVe file\n",
        "# word_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
        "\n",
        "# # Function to obtain word vectors for a sentence\n",
        "# def get_sentence_embedding(sentence):\n",
        "#     words = word_tokenize(sentence)  # Tokenize sentence into words\n",
        "#     word_embeddings = []\n",
        "#     for word in words:\n",
        "#         try:\n",
        "#             word_embeddings.append(word_vectors[word])  # Get GloVe vector for word\n",
        "#         except KeyError:\n",
        "#             # Handle out-of-vocabulary words\n",
        "#             pass\n",
        "#     if len(word_embeddings) == 0:\n",
        "#         # Handle case where all words are out-of-vocabulary\n",
        "#         return np.zeros(word_vectors.vector_size)\n",
        "#     else:\n",
        "#         # Aggregate word vectors (take sum)\n",
        "#         return np.sum(word_embeddings, axis=0)\n",
        "\n",
        "# # Apply to comment and parent comment\n",
        "# comment_vectors = train_bal['comment'].apply(get_sentence_embedding)\n",
        "# parent_comment_vectors = train_bal['parent_comment'].apply(get_sentence_embedding)\n",
        "\n",
        "# Convert comment_vectors and parent_comment_vectors to NumPy arrays\n",
        "# comment_vectors_np = np.array(comment_vectors.tolist())\n",
        "# parent_comment_vectors_np = np.array(parent_comment_vectors.tolist())\n",
        "\n",
        "# # Save the NumPy arrays to files\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/comment_vectors.npy', comment_vectors_np)\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/parent_comment_vectors.npy', parent_comment_vectors_np)"
      ],
      "metadata": {
        "id": "jWiKndy5C6S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved NumPy arrays\n",
        "# comment_vectors = np.load('/content/drive/MyDrive/Sarcasm_Data/comment_vectors.npy')\n",
        "# parent_comment_vectors = np.load('/content/drive/MyDrive/Sarcasm_Data/parent_comment_vectors.npy')"
      ],
      "metadata": {
        "id": "QRV3Uy9UICj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "word2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/Sarcasm_Data/word2vec.model')"
      ],
      "metadata": {
        "id": "yjY58jUxe8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string, time, nltk\n",
        "import numpy as np\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as stopwordprovider\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9h26jisUeJc",
        "outputId": "30711c69-eecc-42fd-c65c-ff34e894f829"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrx-2vDKUeQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_bal['label']\n",
        "X = train_bal.drop(columns=['label'])"
      ],
      "metadata": {
        "id": "lPn8BNwCIKOu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "t8zGxd5oHyaq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing train data"
      ],
      "metadata": {
        "id": "_TWTtb8J7v6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_text = X_train[['comment', 'parent_comment']]\n",
        "# X_numeric = X_train[['word_count', 'capital_letters_count', 'total_punctuation']]"
      ],
      "metadata": {
        "id": "A7fWClefDnS7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_numeric.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf5xr8wmP7VU",
        "outputId": "b8940c5e-171a-413d-907b-c8a3849b2ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "646894"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_text.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hkhJcjtRiO3",
        "outputId": "923063fb-13f7-4639-b24d-60fda0937db1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "646894"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000  # Maximum number of words to keep based on word frequency\n",
        "max_seq_length = 100  # Maximum length of sequences"
      ],
      "metadata": {
        "id": "Xv1CbJ_JECdP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, concatenate, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=False)\n",
        "tokenizer.fit_on_texts(X_text['comment'] + ' ' + X_text['parent_comment'])"
      ],
      "metadata": {
        "id": "vkKhwNpGEVpo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Possibly tokenize differenlty for comment and parent comment****"
      ],
      "metadata": {
        "id": "INiTsNAZNRpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_comment_seq = tokenizer.texts_to_sequences(X_text['comment'])\n",
        "X_text_parent_comment_seq = tokenizer.texts_to_sequences(X_text['parent_comment'])"
      ],
      "metadata": {
        "id": "8U6rSQhHE19Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_comment_pad = pad_sequences(X_text_comment_seq, maxlen=max_seq_length)\n",
        "X_text_parent_comment_pad = pad_sequences(X_text_parent_comment_seq, maxlen=max_seq_length)"
      ],
      "metadata": {
        "id": "AWS2YuvXFO2o"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler = StandardScaler()\n",
        "# X_numeric_scaled = scaler.fit_transform(X_numeric)"
      ],
      "metadata": {
        "id": "PsZ11UoIGxPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2vec model trained on our text data"
      ],
      "metadata": {
        "id": "vToKMYEOrzSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "word2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/Sarcasm_Data/word2vec.model')"
      ],
      "metadata": {
        "id": "PimYeb7fr10c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word2vec.wv.most_similar('ok')\n",
        "similar_words"
      ],
      "metadata": {
        "id": "tOYksVAZsI2L",
        "outputId": "dd40d9d9-2acd-419b-c265-e68f71613190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('okay', 0.9446449279785156),\n",
              " ('OK', 0.9253052473068237),\n",
              " ('alright', 0.7982171773910522),\n",
              " ('fine', 0.6672182083129883),\n",
              " ('perfectly_fine', 0.642020046710968),\n",
              " ('cool', 0.5878971815109253),\n",
              " ('justified', 0.5626591444015503),\n",
              " ('wrong', 0.5318138599395752),\n",
              " ('understandable', 0.5288909077644348),\n",
              " ('unacceptable', 0.4947114884853363)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word_index = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key, start=1)}\n"
      ],
      "metadata": {
        "id": "rHyHDGozvsrs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# Assume word2vec is your pre-trained Word2Vec model\n",
        "# Assume word_index is a dictionary mapping from word to integer index\n",
        "# Assume max_words is the size of your vocabulary, and word2vec.vector_size is the size of each word vector\n",
        "\n",
        "embedding_dim = word2vec.vector_size\n",
        "max_words = len(word_index) + 1  # Adjust based on whether you start indexing from 0 or 1\n",
        "\n",
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "# # Populate the embedding matrix\n",
        "# for word, i in word_index.items():\n",
        "#     if i < max_words:\n",
        "#         if word in word2vec.wv:\n",
        "#             # Words not found in the embedding index will be all zeros\n",
        "#             embedding_matrix[i] = word2vec.wv[word]\n",
        "\n"
      ],
      "metadata": {
        "id": "9L0nq3qru1Up"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "# Populate the embedding matrix using the tokenizer's word_index\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words:  # Ensure you don't exceed the max_words limit\n",
        "        if word in word2vec.wv:\n",
        "            embedding_matrix[i] = word2vec.wv[word]\n"
      ],
      "metadata": {
        "id": "B-Y5VsEnTatF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim"
      ],
      "metadata": {
        "id": "CGmd2iDQw-nf",
        "outputId": "6bbcf8a3-ceb7-4917-93c8-5c015b0d0fd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Test Data"
      ],
      "metadata": {
        "id": "N85IjdJow5tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_test = X_test[['comment', 'parent_comment']]\n",
        "# X_numeric_test = X_test[['word_count', 'capital_letters_count', 'total_punctuation']]"
      ],
      "metadata": {
        "id": "mt5Xai4-OmAL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_comment_seq_test = tokenizer.texts_to_sequences(X_text_test['comment'])\n",
        "X_text_parent_comment_seq_test = tokenizer.texts_to_sequences(X_text_test['parent_comment'])"
      ],
      "metadata": {
        "id": "V-qXRqiUO-Cn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_comment_pad_test = pad_sequences(X_text_comment_seq_test, maxlen=max_seq_length)\n",
        "X_text_parent_comment_pad_test = pad_sequences(X_text_parent_comment_seq_test, maxlen=max_seq_length)"
      ],
      "metadata": {
        "id": "LahsOuvVO4KC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout, Bidirectional, LSTM\n",
        "# from keras.models import Model\n",
        "# from keras import optimizers\n",
        "# # from kerastuner import HyperModel\n",
        "\n",
        "# def build_model1(hp):\n",
        "#     comment_input = Input(shape=(max_seq_length,), name='comment_input')\n",
        "#     parent_comment_input = Input(shape=(max_seq_length,), name='parent_comment_input')\n",
        "\n",
        "#     embedding_layer = Embedding(input_dim=max_words,\n",
        "#                                 output_dim=embedding_dim,\n",
        "#                                 weights=[embedding_matrix],\n",
        "#                                 input_length=max_seq_length,\n",
        "#                                 trainable=True)\n",
        "\n",
        "#     comment_embedding = embedding_layer(comment_input)\n",
        "#     parent_comment_embedding = embedding_layer(parent_comment_input)\n",
        "\n",
        "#     concatenated_embeddings = concatenate([comment_embedding, parent_comment_embedding], axis=1)\n",
        "\n",
        "#     rnn_layer = SimpleRNN(units=hp.Int('rnn_units', min_value=64, max_value=256, step=64),\n",
        "#                           dropout=0.2)(concatenated_embeddings)\n",
        "\n",
        "#     dense = Dense(hp.Int('dense_units', min_value=64, max_value=128, step=32), activation='relu')(rnn_layer)\n",
        "#     output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "#     model = Model(inputs=[comment_input, parent_comment_input], outputs=output)\n",
        "\n",
        "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "#     model.compile(optimizer=optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "#                   loss='binary_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     return model"
      ],
      "metadata": {
        "id": "XqGKGFTMUeTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, SimpleRNN, Dense, concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define input shapes\n",
        "comment_input = Input(shape=(max_seq_length,), name='comment_input')\n",
        "parent_comment_input = Input(shape=(max_seq_length,), name='parent_comment_input')\n",
        "\n",
        "# Define embedding layer\n",
        "embedding_layer = Embedding(input_dim=max_words,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_seq_length,\n",
        "                            trainable=True)\n",
        "\n",
        "# Apply embedding layer to inputs\n",
        "comment_embedding = embedding_layer(comment_input)\n",
        "parent_comment_embedding = embedding_layer(parent_comment_input)\n",
        "\n",
        "# Concatenate embeddings\n",
        "concatenated_embeddings = concatenate([comment_embedding, parent_comment_embedding], axis=1)\n",
        "\n",
        "# Define SimpleRNN layer\n",
        "rnn_layer = SimpleRNN(units=128, dropout=0.2)(concatenated_embeddings)\n",
        "\n",
        "# Define output layer\n",
        "output = Dense(1, activation='sigmoid')(rnn_layer)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=[comment_input, parent_comment_input], outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define model checkpoint callback to save the best model\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x={'comment_input': X_text_comment_pad, 'parent_comment_input': X_text_parent_comment_pad},\n",
        "    y=y_train,\n",
        "    validation_data=({'comment_input': X_text_comment_pad_test, 'parent_comment_input': X_text_parent_comment_pad_test}, y_test),\n",
        "    epochs=3,\n",
        "    batch_size=32,\n",
        "    callbacks=[model_checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCEss3BdUeVa",
        "outputId": "c9476b74-d8d9-424d-bc46-35aa54925d5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "20216/20216 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.5332"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r20216/20216 [==============================] - 3905s 193ms/step - loss: 0.6916 - accuracy: 0.5332 - val_loss: 0.6835 - val_accuracy: 0.5564\n",
            "Epoch 2/3\n",
            "20216/20216 [==============================] - 3810s 188ms/step - loss: 0.6857 - accuracy: 0.5501 - val_loss: 0.6831 - val_accuracy: 0.5530\n",
            "Epoch 3/3\n",
            "20216/20216 [==============================] - 3982s 197ms/step - loss: 0.6855 - accuracy: 0.5528 - val_loss: 0.6992 - val_accuracy: 0.5037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test data\n",
        "best_model = load_model('best_model.h5')  # Load the best model saved by the model checkpoint callback\n",
        "loss, accuracy = best_model.evaluate({'comment_input': X_text_comment_pad_test, 'parent_comment_input': X_text_parent_comment_pad_test}, y_test)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "5ZiQeGROUeYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kNqJKIUUea1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sxdi-s8gUedC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBE_qOsEUefe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I53A2qSNUeh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxblnpkCUekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHbyEhOfUenC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hRCN2dMUepY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D, SimpleRNN\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Output directory name\n",
        "output_dir = 'model_output/rnn'\n",
        "\n",
        "# Training\n",
        "epochs = 16\n",
        "batch_size = 128\n",
        "\n",
        "# Word embedding properties\n",
        "n_dim = 100  # assuming your word embeddings are of dimension 100\n",
        "max_review_length = 100  # adjust according to your requirements\n",
        "pad_type = trunc_type = 'pre'\n",
        "drop_embed = 0.2\n",
        "\n",
        "# RNN layer architecture\n",
        "n_rnn = 256\n",
        "drop_rnn = 0.2\n",
        "\n",
        "# Load word embeddings\n",
        "word2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/Sarcasm_Data/word2vec.model')\n",
        "\n",
        "# Function to convert text to word embeddings\n",
        "def text_to_embeddings(text):\n",
        "    embeddings = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            embeddings.append(word2vec.wv[word])\n",
        "        except KeyError:\n",
        "            # Handle out-of-vocabulary words\n",
        "            pass\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Preprocess data\n",
        "train_bal['comment_embedding'] = train_bal['comment'].apply(text_to_embeddings)\n",
        "train_bal['parent_comment_embedding'] = train_bal['parent_comment'].apply(text_to_embeddings)\n",
        "\n",
        "# Convert to padded sequences\n",
        "x_train = pad_sequences(train_bal['comment_embedding'], maxlen=max_review_length, padding=pad_type, truncating=trunc_type, dtype='float32')\n",
        "x_valid = pad_sequences(train_bal['parent_comment_embedding'], maxlen=max_review_length, padding=pad_type, truncating=trunc_type, dtype='float32')\n",
        "y_train = train_bal['label'].values\n",
        "\n",
        "# Design neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word2vec.wv), n_dim, input_length=max_review_length, weights=[word2vec.wv.vectors], trainable=False))\n",
        "model.add(SpatialDropout1D(drop_embed))\n",
        "model.add(SimpleRNN(n_rnn, dropout=drop_rnn))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "# Define model checkpoint\n",
        "model_checkpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2, callbacks=[model_checkpoint])\n"
      ],
      "metadata": {
        "id": "hfXS38GsRqc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_hat = model.predict_proba(x_valid)\n",
        "roc_auc = roc_auc_score(y_valid, y_hat)\n",
        "print(\"ROC AUC:\", roc_auc)"
      ],
      "metadata": {
        "id": "1wBphjRmSvsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xp_BDPliSxdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "leCG0ckxSxgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tjmeT1bSxhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zh-7cQQfSvx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgmm6pHLSv1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word2vec.wv.most_similar('ok')\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9BPypJXfHK1",
        "outputId": "da1cc01e-4406-4dc6-dcfb-58f831b12916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('okay', 0.9446449279785156),\n",
              " ('OK', 0.9253052473068237),\n",
              " ('alright', 0.7982171773910522),\n",
              " ('fine', 0.6672182083129883),\n",
              " ('perfectly_fine', 0.642020046710968),\n",
              " ('cool', 0.5878971815109253),\n",
              " ('justified', 0.5626591444015503),\n",
              " ('wrong', 0.5318138599395752),\n",
              " ('understandable', 0.5288909077644348),\n",
              " ('unacceptable', 0.4947114884853363)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key, start=1)}"
      ],
      "metadata": {
        "id": "o_U7WAPUfwGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comment_vectors.shape)\n",
        "print(parent_comment_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xOzAlyxJh__",
        "outputId": "9e6010da-d607-440a-f3ea-e0aac47dd63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(808618, 100)\n",
            "(808618, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(comment_vectors[0])\n",
        "print('-'*100)\n",
        "print(parent_comment_vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCj-7Jj3JjOJ",
        "outputId": "4b6360ec-7cfd-40b8-8b43-c5f34e6cdb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-7.95173943e-01  3.14127636e+00  2.78570318e+00 -5.05811024e+00\n",
            " -5.42068005e-01  3.63985920e+00 -1.27075803e+00  1.66128492e+00\n",
            "  3.84848982e-01 -5.39201796e-01  1.56411278e+00 -4.96732265e-01\n",
            "  3.14057279e+00 -1.09438193e+00  1.24003977e-01 -4.92372990e+00\n",
            "  1.95057189e+00  1.45621693e+00 -5.86305523e+00  3.62457490e+00\n",
            "  2.70052624e+00  2.39906836e+00  1.30075109e+00 -1.94248295e+00\n",
            "  4.43695879e+00  5.67565024e-01 -4.04613018e+00 -4.17990780e+00\n",
            "  2.51732898e+00 -2.02238202e+00 -4.93887007e-01  7.83402014e+00\n",
            "  1.92612505e+00  3.37638760e+00  1.19841099e+00  2.33827353e+00\n",
            " -1.06820077e-01  4.66233015e+00  1.24715900e+00 -3.10010982e+00\n",
            " -3.65968680e+00 -2.88997602e+00  4.48291922e+00 -5.16998005e+00\n",
            " -4.32980001e-01 -1.64014041e-01  3.90807080e+00 -3.62764406e+00\n",
            "  1.86569667e+00 -1.19882402e+01  2.76568204e-01 -2.21814656e+00\n",
            "  2.40661955e+00  1.25513506e+01 -2.04660821e+00 -3.07382088e+01\n",
            " -9.85192060e-02  5.29608965e-01  1.64236794e+01  8.33390999e+00\n",
            " -9.90572035e-01  1.01262293e+01 -5.51144171e+00  1.27350235e+00\n",
            "  9.80021000e+00 -2.77950025e+00  8.05648518e+00  4.56785011e+00\n",
            "  1.56188107e+00  4.03987914e-01  1.38067400e+00 -3.48511577e+00\n",
            "  5.78177989e-01 -2.72719693e+00  1.20745397e+00  2.79291177e+00\n",
            " -6.48575962e-01 -2.50220001e-02 -1.01622009e+01  1.89497089e+00\n",
            "  7.94438982e+00  9.73711014e-01 -5.77689505e+00  2.54203796e-01\n",
            " -1.82186203e+01 -4.89910984e+00 -1.17492509e+00 -3.34603620e+00\n",
            " -3.99891782e+00 -5.57610989e+00 -4.09482062e-01 -4.36970425e+00\n",
            "  7.25854039e-01  1.25175190e+00 -7.29954386e+00  5.87913990e-01\n",
            " -4.51064205e+00 -4.12998629e+00  5.59506273e+00  3.52249193e+00]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[  -6.45141554   10.62220478   21.36541939  -22.61557961    0.27064675\n",
            "   17.80076981   -8.00616646   10.81078529   -4.19529915   -0.75270176\n",
            "    5.97754478    3.27354002   14.99562359    0.91880578    9.20004272\n",
            "  -22.17715454   13.84591961    5.55279732  -35.88007736   15.58642387\n",
            "   19.8537159    -7.52998352   18.32880783    5.70689631   14.37128544\n",
            "   -3.0309062    -9.42590237  -32.03544998   13.90989971   -9.86495304\n",
            "   -1.73210871   37.07268524   -7.81048536    4.87950039   -1.54337645\n",
            "   21.02444267   -2.0448339    15.89016056   -2.99823999  -11.50785637\n",
            "  -30.78156853  -13.59989357    6.79149914  -28.43291855   -9.75974464\n",
            "   -2.4178257    12.80748844  -23.69897079   -4.22813702  -56.01848221\n",
            "   -1.35161519   -7.73641968    9.85395908   79.48160553  -15.84952736\n",
            " -162.96508789   -4.84318209  -16.68033409  106.38171387   35.82902908\n",
            "  -18.16607285   51.0105896   -16.30660629    5.14722967   55.04941559\n",
            "   -1.1661284    29.90062714   25.46911049    9.5597105    -9.28812408\n",
            "   -6.18693876  -28.31957626   -9.87862587  -24.71577072    9.68878269\n",
            "    2.68926978  -10.54745674   -0.35068822  -57.68596268   -4.61003685\n",
            "   42.05381393    1.83460593  -33.23879242    5.76612616  -92.20313263\n",
            "  -14.30464268    6.5310154    -7.74765968   -9.81037807  -24.44003105\n",
            "   -4.75809383  -15.41175747   -5.88770533    9.15144444  -39.472229\n",
            "   -2.27934933  -13.94301128  -15.30041313   29.74198723    8.53223038]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the embeddings to train_bal DataFrame\n",
        "train_bal['comment_embedding'] = comment_vectors.tolist()\n",
        "train_bal['parent_comment_embedding'] = parent_comment_vectors.tolist()\n",
        "\n",
        "# Split the data into features and target\n",
        "X = train_bal[['comment_embedding']]  # Features\n",
        "y = train_bal['label']  # Target\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X['comment_embedding'] = X['comment_embedding'].apply(np.array)\n",
        "X['parent_comment_embedding'] = X['parent_comment_embedding'].apply(np.array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5aDoMnOWsx",
        "outputId": "34b5a37c-64ba-46ca-88ef-94ff41c6dbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-962efed96f6d>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['comment_embedding'] = X['comment_embedding'].apply(np.array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=22)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = np.vstack(X_train['comment_embedding'].to_numpy())\n",
        "X_val_scaled = np.vstack(X_val['comment_embedding'].to_numpy())\n",
        "X_train = scaler.fit_transform(X_train_scaled)\n",
        "X_val = scaler.transform(np.vstack(X_val['comment_embedding'].to_numpy()))"
      ],
      "metadata": {
        "id": "FsqlDoZNONcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)"
      ],
      "metadata": {
        "id": "6vB3SEEPQx9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save X_train, X_val, y_train, and y_val\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/X_train.npy', X_train)\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/X_val.npy', X_val)\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/y_train.npy', y_train)\n",
        "# np.save('/content/drive/MyDrive/Sarcasm_Data/y_val.npy', y_val)"
      ],
      "metadata": {
        "id": "r7wWNbf-SjiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load X_train, X_val, y_train, and y_val\n",
        "# X_train = np.load('/content/drive/MyDrive/Sarcasm_Data/X_train.npy')\n",
        "# X_val = np.load('/content/drive/MyDrive/Sarcasm_Data/X_val.npy')\n",
        "# y_train = np.load('/content/drive/MyDrive/Sarcasm_Data/y_train.npy')\n",
        "# y_val = np.load('/content/drive/MyDrive/Sarcasm_Data/y_val.npy')\n",
        "\n",
        "# # Load the saved NumPy arrays\n",
        "# comment_vectors = np.load('/content/drive/MyDrive/Sarcasm_Data/comment_vectors.npy')\n",
        "# parent_comment_vectors = np.load('/content/drive/MyDrive/Sarcasm_Data/parent_comment_vectors.npy')"
      ],
      "metadata": {
        "id": "NlbM9qsrSm8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(units=128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okq0KKteQbWN",
        "outputId": "5c13d7a6-7579-43c4-c5b7-c6af41110e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               66560     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66689 (260.50 KB)\n",
            "Trainable params: 66689 (260.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_iiaKYiJw9X",
        "outputId": "7ec22ee4-3bfd-4760-a2ef-a6ed848a68bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20216/20216 [==============================] - 167s 8ms/step - loss: 0.6857 - accuracy: 0.5482 - val_loss: 0.6790 - val_accuracy: 0.5657\n",
            "Epoch 2/10\n",
            "20216/20216 [==============================] - 164s 8ms/step - loss: 0.6796 - accuracy: 0.5652 - val_loss: 0.6784 - val_accuracy: 0.5670\n",
            "Epoch 3/10\n",
            "20216/20216 [==============================] - 162s 8ms/step - loss: 0.6784 - accuracy: 0.5688 - val_loss: 0.6753 - val_accuracy: 0.5752\n",
            "Epoch 4/10\n",
            "20216/20216 [==============================] - 157s 8ms/step - loss: 0.6769 - accuracy: 0.5702 - val_loss: 0.6726 - val_accuracy: 0.5796\n",
            "Epoch 5/10\n",
            "20216/20216 [==============================] - 157s 8ms/step - loss: 0.6699 - accuracy: 0.5854 - val_loss: 0.6668 - val_accuracy: 0.5920\n",
            "Epoch 6/10\n",
            "20216/20216 [==============================] - 156s 8ms/step - loss: 0.6662 - accuracy: 0.5922 - val_loss: 0.6628 - val_accuracy: 0.5975\n",
            "Epoch 7/10\n",
            "20216/20216 [==============================] - 161s 8ms/step - loss: 0.6630 - accuracy: 0.5961 - val_loss: 0.6630 - val_accuracy: 0.5981\n",
            "Epoch 8/10\n",
            "20216/20216 [==============================] - 158s 8ms/step - loss: 0.6759 - accuracy: 0.5724 - val_loss: 0.6773 - val_accuracy: 0.5697\n",
            "Epoch 9/10\n",
            "20216/20216 [==============================] - 157s 8ms/step - loss: 0.6757 - accuracy: 0.5725 - val_loss: 0.6730 - val_accuracy: 0.5801\n",
            "Epoch 10/10\n",
            "20216/20216 [==============================] - 157s 8ms/step - loss: 0.6707 - accuracy: 0.5838 - val_loss: 0.6680 - val_accuracy: 0.5876\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7de1d7f01210>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on validation data\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print('Validation Loss:', loss)\n",
        "print('Validation Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuN0A6p-RQC3",
        "outputId": "d83b4148-856e-4bf0-afac-75e20be140e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5054/5054 [==============================] - 28s 5ms/step - loss: 0.6680 - accuracy: 0.5876\n",
            "Validation Loss: 0.6679925918579102\n",
            "Validation Accuracy: 0.5876493453979492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9Ut8C6UyUt",
        "outputId": "4cce53c1-7318-4fea-f73d-4add78fee18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assume train_bal contains the DataFrame with comment embeddings\n",
        "\n",
        "# Define function to convert data to sparse matrix\n",
        "def convert_to_sparse(data):\n",
        "    sparse_data = [csr_matrix(item) for item in data]\n",
        "    return sparse_data\n",
        "\n",
        "# Convert comment embeddings to sparse matrices\n",
        "train_bal['comment_embedding'] = convert_to_sparse(train_bal['comment_embedding'])\n",
        "\n",
        "# Split the data into features and target\n",
        "X = train_bal[['comment_embedding']]  # Features\n",
        "y = train_bal['label']  # Target\n",
        "\n",
        "# Split data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=22)\n",
        "\n",
        "# Define function to scale and convert data to sparse matrix\n",
        "def scale_and_convert_to_sparse(data):\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "    sparse_data = convert_to_sparse(scaled_data)\n",
        "    return sparse_data\n",
        "\n",
        "# Scale and convert comment embeddings to sparse matrices for train and validation sets\n",
        "X_train = scale_and_convert_to_sparse(X_train['comment_embedding'])\n",
        "X_val = scale_and_convert_to_sparse(X_val['comment_embedding'])\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(units=128, input_shape=(X_train[0].shape[1],), activation='relu'),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print('Validation Loss:', loss)\n",
        "print('Validation Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "tWn1sjuCUzcT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}